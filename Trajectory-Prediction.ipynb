{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Prediction with LSTM\n",
    "### CS230 - Deep Learning -  Final Submission. \n",
    "#### Mitchell Dawson, Benjamin Goeing, Tyler Hughes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    " In this project, we training an RNN to predict the trajectory of objects (pedestrians, bikers, cars, etc.), as they move through a scene and interact with one another. Our model could be used to predict movements of crowds of people and vehicles given an overhead image of a scene. This may have potential applications in helping to make public spaces less susceptible to crowding or accidents, improving control of autonomous vehicles, or video surveillance.  We provide several sources of additional input to the RNN to improve its performance.  For example, information related to the tracked subject's location and trajectory, the scene layout, and the presence of other people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We are leveraging the Stanford Drone dataset, which contains a large number of overhead images of crowded spaces on Stanford campus. (http://cvgl.stanford.edu/projects/uav_data/). Here are some examples of drone footage from this dataset:\n",
    "\n",
    "<img src=\"images/bookstore.jpg?raw=true\" width=\"250\"/>  <img src=\"images/deathCircle.jpg?raw=true\" width=\"250\"/>   \n",
    "\n",
    "Each scene in the dataset also contains csv files with rows in the form of (f,o,x,y).  Here 'f' is the frame number, 'o' is the unique object identifier (which person is being tracked) and x,y are the x and y coordinates (not normalized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports\n",
    "Our LSTM model is built with Pytorch.\n",
    "\n",
    "We will be importing helper modules for processing the dataset and loading an LSTM trajectory tracker class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module import (add here as necessary)\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from simple_processing import load_simple_array\n",
    "from lstm import TrajectoryPredictor\n",
    "from linear_error import compute_linear_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants\n",
    "We have to specify the number of frames to observe before predicting.\n",
    "Also, for training the LSTM, we must specify the batch size and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "Nf = 10         # number of frames to observe before making prediction\n",
    "batch_size = 4  # TrajectoryPredictor training batch size\n",
    "num_epochs = 10 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Training Data\n",
    "We will load in and process the drone dataset.\n",
    "\n",
    "For now, we will just load (x,y) pairs of positions at a series of frames for each person in the scene.  We will use both the scene information and presence of other people in the frame later.  We normalize the (x,y) positions to be between -1 and 1\n",
    "\n",
    "The data is loaded into a pytorch dataset for feeding into the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "    train_trajectories = []\n",
    "    for filename in os.listdir('train/stanford/annotations/'):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        train_trajectories += load_simple_array('train/stanford/annotations/' + filename)\n",
    "    \n",
    "    dev_trajectories = []\n",
    "    for filename in os.listdir('dev/stanford/annotations/'):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        dev_trajectories += load_simple_array('dev/stanford/annotations/' + filename)\n",
    "\n",
    "    test_trajectories = []\n",
    "    for filename in os.listdir('test/stanford/annotations/'):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        test_trajectories += load_simple_array('test/stanford/annotations/' + filename)\n",
    "\n",
    "    return train_trajectories, dev_trajectories, test_trajectories\n",
    "\n",
    "\n",
    "train_trajectories, dev_trajectories, test_trajectories = load_data()\n",
    "\n",
    "np_train_trajectories = np.stack(train_trajectories)\n",
    "\n",
    "np_train_data = np_train_trajectories[:,:Nf,:]\n",
    "np_train_target = np_train_trajectories[:,Nf:,:]\n",
    "\n",
    "train_data_tensor = torch.Tensor(np_train_data)\n",
    "train_target_tensor = torch.Tensor(np_train_target)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_tensor, train_target_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for subsequent sections, we have preloaded and processed the dataset to include the extra information we wish to supply for the LSTM, we do not include that code in this notebook but it is all in the file 'simple_processing.py'\n",
    "\n",
    "Here we print out the trajectory of the first person in the training set. The first $N_f$ ($x$,$y$) pairs are for observation. The second $N_f$ ($x$,$y$) pairs are those we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      " 0.0120  0.8285\n",
      " 0.0120  0.8285\n",
      " 0.0134  0.8382\n",
      " 0.0219  0.8382\n",
      " 0.0318  0.8405\n",
      " 0.0403  0.8424\n",
      " 0.0502  0.8442\n",
      " 0.0598  0.8465\n",
      " 0.0697  0.8507\n",
      " 0.0792  0.8526\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      " 0.0905  0.8526\n",
      " 0.1011  0.8526\n",
      " 0.1124  0.8549\n",
      " 0.1234  0.8563\n",
      " 0.1347  0.8581\n",
      " 0.1457  0.8600\n",
      " 0.1563  0.8600\n",
      " 0.1676  0.8600\n",
      " 0.1772  0.8563\n",
      " 0.1885  0.8535\n",
      "[torch.FloatTensor of size 10x2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM\n",
    "Now that we have our data loaded into a pytorch dataset and data loader, we are ready to train the RNN.  We have opted for an LSTM model, initially with a hidden state size of 2, although we will expand on this model later.\n",
    "\n",
    "<img src=\"images/LSTM_xy.png?raw=true\" width=\"500\">   \n",
    "\n",
    "We have implemented a TrajectoryPredictor() class that trains an LSTM on the pytorch data loader.\n",
    "TrajectoryPredictor takes as initial parameters: input dimension (2 here because we have x,y inputs), output dimension (2 here because x,y output predictions), and batch size.\n",
    "\n",
    "We commented out the code to initialize a trajectory predictor and train it.  Instead, we will load a pre-trained model that we trained on FloydHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1. mean loss: 1.82455535136\n",
      "epoch 2. mean loss: 0.546421991492\n",
      "epoch 3. mean loss: 0.386153386321\n",
      "epoch 4. mean loss: 0.323843483411\n",
      "epoch 5. mean loss: 0.289107988643\n",
      "epoch 6. mean loss: 0.266656400561\n",
      "epoch 7. mean loss: 0.250179112599\n",
      "epoch 8. mean loss: 0.236991802132\n"
     ]
    }
   ],
   "source": [
    "# p = TrajectoryPredictor(2, 2, batch_size)\n",
    "# p.train(train_loader, num_epochs)\n",
    "\n",
    "# LOAD ONLY XY MODEL HERE!!!\n",
    "# p_xy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "With our model trained, we predict on some validation trajectories to get a sense how well our LSTM performs.  \n",
    "\n",
    "<img src=\"images/test_predict.png?raw=true\" width=\"150\">   \n",
    "\n",
    "We use a loss function of\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{MN}\\sum_{i=1}^M \\sum_{j=1}^N \\sqrt{\\big(x^{(i)}_j - \\bar{x}^{(i)}_j\\big)^2 + \\big(y^{(i)}_j - \\bar{y}^{(i)}_j\\big)^2}  $$\n",
    "\n",
    "Where $i$ is the trajectory number and $j$ is the frame number.  $x$ and $y$ are the predicted coordinates and $\\bar{x}$ and $\\bar{y}$ are the true coordinates.\n",
    "\n",
    "This loss function gives us the mean, average displacement error for each frame in each trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_dev_trajectories = np.stack(dev_trajectories)\n",
    "\n",
    "np_dev_data = np_dev_trajectories[:,:Nf,:]\n",
    "np_dev_target = np_dev_trajectories[:,Nf:,:]\n",
    "\n",
    "dev_data_tensor = torch.Tensor(np_dev_data)\n",
    "dev_target_tensor = torch.Tensor(np_dev_target)\n",
    "\n",
    "dev_dataset = torch.utils.data.TensorDataset(dev_data_tensor, dev_target_tensor)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.534271395503\n"
     ]
    }
   ],
   "source": [
    "p_xy.test(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare with the predictions of a linear model on the same trajectory, where the (x,y) coordinates of a future time frame are estimated by extrapolating from the person's velocity at the final observation time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.128018440883\n"
     ]
    }
   ],
   "source": [
    "compute_linear_error(dev_trajectories, Nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we have to include more information in order to improve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Velocity\n",
    "We suspect that with just the (x,y) data, once the model starts producing errors in the trajectory prediction, these errors begin to compound because the subject walks off from the correct path.  In order to bias our model against this, we decided to add in the subject's velocity at the final time step (not updated after prediction).  This will have the effect of producing predictions that are more linear in the direction of the original training trajectory.\n",
    "\n",
    "We compute the final velocity with a simple finite difference formula, where $N$ is the final training frame and $\\Delta t$ is the frame rate\n",
    "\n",
    "$$v_x = \\frac{x_N - x_{N-1}}{\\Delta t}$$\n",
    "\n",
    "\n",
    "$$v_y = \\frac{y_N - y_{N-1}}{\\Delta t}$$\n",
    "\n",
    "Let us load our pretrained model and predict on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD VELOCITY DATA IN:\n",
    "# p_xyv = load_v\n",
    "# PREDICT ON DEV SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model performed better than with just $x$ and $y$, we still have some ways to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer\n",
    "In order to add dimensionality to our LSTM and also allow for the model to learn an embedding for the input vectors, we added a dense layer between our $x$,$y$,$\\vec{v}$ data and our LSTM input:\n",
    "\n",
    "<img src=\"images/LSTM_xyv_dense.png?raw=true\" width=\"500\">   \n",
    "\n",
    "This dense layer takes the 4 dimensional input and transforms it to an $LSTM_{size}$ dimensional input.  We chose $LSTM_{size}$ to be 20 after trying several values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD VELOCITY+DENSE DATA IN:\n",
    "# p_xyv_dense = load_v\n",
    "# PREDICT ON DEV SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the dense layer slightly improved performance of the model, but we still only have information from a single person's trajectory. Therefore, our model is not yet able to incorporate important scene or crowd information.  We add this in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy Grid\n",
    "The input training data has a set of ($x$,$y$) pairs at different frames, but also contains information about all of the subjects in the scene at a given frame.  \n",
    "\n",
    "Therefore, we did some preprocessing of the initial dataset to give, for each trajectory, not only ($x$,$y$) data, but also an occupancy grid representing all of the other people's locations in the scene at a given time.\n",
    "\n",
    "The form of this occupancy grid was an $N_o\\times N_o$ numpy array with a $0$ where there is no person and $1$ where there is a person in the scene.  We fed this numpy array directly into the LSTM along with the $x$, $y$ and velocity information.\n",
    "\n",
    "<img src=\"images/LSTM_others.png?raw=true\" width=\"500\">   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD OTHERS DATA IN:\n",
    "# p_others = load_v\n",
    "# PREDICT ON DEV SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputting the data in this form seemed to have only a marginal effect on the performance of the model.  It's possible that the numpy array was too large and the LSTMs had trouble inferring any relevant information from this sparse input data.  We will revisit this issue after discussing incorporating scene information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation / Scene Information\n",
    "\n",
    "Our dataset contains several images of the scene from overhead.  We figure that the characteristics of the scene will have a dramatic effect on how people walk around and interact with the scene.  Therefore, we wanted to feed this information to our LSTM in a compact representation.  \n",
    "\n",
    "To make things easier for our model to learn, we needed to first abstract away a lot of the details in the images.  We used MIT's Lableme (http://labelme.csail.mit.edu/Release3.0/) tool to manually segment each image and classify each pixel by class:\n",
    "- a) road\n",
    "- b) sidewalk\n",
    "- c) grass\n",
    "- d) inaccessible (describing objects such as building walls, trees etc.) \n",
    "\n",
    "A matlab script was written to then make sure there were no overlaps or unlabelled pixels.\n",
    "\n",
    "Here is an example of the scene labeling and segmentation process:\n",
    "\n",
    "<img src=\"images/imageSeg.png?raw=true\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Approach\n",
    "\n",
    "In the previous section, we had issues feeding in the occupancy grid directly to our LSTM.  This was likely bceause of its large size compared to the relevant information contained in the array.  \n",
    "\n",
    "Therefore, we decided that the best approach would be to feed any further image/scene data into the LSTM after being processed through a CNN.  \n",
    "\n",
    "We used a pretrained network, AlexNet https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf, as our CNN, since we decided that it would be too computationally intensive to train the CNN kernels.\n",
    "\n",
    "In addition to the segmented background image and the occupancy grid, we also fed the CNN in a numpy array representing where the tracked person is in the scene.  This was to ensure that the spatial relationship between the tracked person and this other information was encoded in the same way by the CNN.\n",
    "\n",
    "<img src=\"images/LSTM_full.png?raw=true\" width=\"500\">\n",
    "\n",
    "### more details here\n",
    "\n",
    "We load in this pre-trained model from floydhub and test it on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD FULL DATA IN:\n",
    "# p_full = load_v\n",
    "# PREDICT ON DEV SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this model performs\n",
    "### How does it perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "\"The notebook should at least contain a thorough explanation of your model, your dataset, your training/validating/testing process, your challenges, an explanation about the hyperparameters, optimization, regularization you choose, the performance of your algorithm, error analysis, some thoughts on future works,\"\n",
    " \n",
    "#### Challenges\n",
    "What are some challenges\n",
    "\n",
    "#### Error Analysis\n",
    "Compare our errors with the stanford vision lab's results.\n",
    "\n",
    "#### Future Works\n",
    "We think it might be useful for future work to explore different CNN architectures for encoding the scene and occupancy data.  We used a pre-trained model because our previous sections were already very computationally intensive, but perhaps training the CNN in conjunction with the LSTM would give better results.  \n",
    "\n",
    "Also, since our segmented image array elements are just indeces corresponding to the scene 'class' of each pixel, there could be several approaches to better use this data.  For example, we could construct separate binary arrays for each of the classes (for example, a separate 'inaccessable' array to feed to the CNN).  Unfortunately, since our training and setup took longer than expected, we did not have enough time to work through these ideas, but we believe that some tweaking could lead to much better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, we have shown that an RNN model can perform reasonably well on trajectory prediction tasks.  We have learned a great deal about how important it is to choose wisely when designing your inputs for the LSTM task.  If the input is too simple then the model will not have enough information to predict trajectories well.  On the other hand, throwing too much data at your LSTM will dramatically increase training time and computational cost, and is not even guarenteed to improve results.  We spent a good amount of time on this project trying to find the middle ground between these two extremes and had some limited success.  Although we did not end up beating the state of the art on these tasks, it was a fruitful learning experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
