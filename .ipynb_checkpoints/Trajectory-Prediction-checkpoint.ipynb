{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Prediction with LSTM\n",
    "### CS230 - Deep Learning -  Final Submission. \n",
    "#### Mitchell Dawson, Benjamin Goeing, Tyler Hughes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Project introduction\n",
    " In this project, we are trying to predict the trajectory of traffic objects (people walking, bikers, cars, etc.), as they move and interact with one another. Our model could be used to predict movements of crowds of people and vehicles given an overhead image of a scene. This may have potential applications in video surveillance, helping to make public spaces less susceptible to crowding or accidents, or improving control of autonomous vehicles. \n",
    "\n",
    "In order to train and test our model, we are leveraging the Stanford Drone dataset, which contains a large number of overhead images of crowded spaces on Stanford campus. (http://cvgl.stanford.edu/projects/uav_data/). Here are some examples of drone footage from this dataset:\n",
    "\n",
    "<img src=\"bookstore.jpg?raw=true\" width=\"250\"> \n",
    "<img src=\"deathCircle.jpg?raw=true\" width=\"250\">   \n",
    "\n",
    "The dataset al contains (x,y) coordinates for each \n",
    "Our final goal for this project is to predict the trajectories of multiple objects in complex scenes with a high accuracy. In a first step, we have implemented a simple linear model as well as a simple LSTM to predict the (x,y) coordinates of objects during the later frames of a scene given the earlier frames of the same scene. We are then evaluating our model by predicting the trajectory of objects in completely unseen scenes. \n",
    "\n",
    "In addition to the provided data, which contains the x,y coordinates of all moving objects for a number of sequential frames, we have used MIT's Lableme (http://labelme.csail.mit.edu/Release3.0/) to further classify each pixel in the background of an image. We have manually labeled the following classes: \n",
    "- a) road\n",
    "- b) sidewalk\n",
    "- c) grass\n",
    "- d) inaccessible (describing objects such as building walls, trees etc.) \n",
    "\n",
    "Here is an example of a labeled image: \n",
    "\n",
    "<img src=\"segImage2.jpg?raw=true\" width=\"250\">\n",
    "\n",
    "As a first step, we are running our model without this additional information to get a sense of the baseline performance. We will then run the model again with the background information to see if and by how much it improves our accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports\n",
    "Our LSTM model is built on Pytorch.\n",
    "\n",
    "We will be importing helper modules for processing the dataset and loading an LSTM trajectory tracker class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module import (add here as necessary)\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from simple_processing import load_simple_array\n",
    "from lstm import TrajectoryPredictor\n",
    "from linear_error import compute_linear_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "Nf = 10         # number of frames to observe before making prediction\n",
    "batch_size = 4  # TrajectoryPredictor training batch size\n",
    "num_epochs = 10 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "We will load in and process the drone dataset.  \n",
    "For now, we will just load (x,y) pairs of positions (normalized between -1 and 1) at a series of frames for each person in the scene.  \n",
    "Our data is taken from the 'stanford' dataset, which contains drone footage from places on Stanford's campus.\n",
    "\n",
    "The data is loaded into a pytorch dataset for feeding into the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "    train_trajectories = []\n",
    "    for filename in os.listdir('train/stanford/annotations/'):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        train_trajectories += load_simple_array('train/stanford/annotations/' + filename)\n",
    "    \n",
    "    dev_trajectories = []\n",
    "    for filename in os.listdir('dev/stanford/annotations/'):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        dev_trajectories += load_simple_array('dev/stanford/annotations/' + filename)\n",
    "\n",
    "    test_trajectories = []\n",
    "    for filename in os.listdir('test/stanford/annotations/'):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "        test_trajectories += load_simple_array('test/stanford/annotations/' + filename)\n",
    "\n",
    "    return train_trajectories, dev_trajectories, test_trajectories\n",
    "\n",
    "\n",
    "train_trajectories, dev_trajectories, test_trajectories = load_data()\n",
    "\n",
    "np_train_trajectories = np.stack(train_trajectories)\n",
    "\n",
    "np_train_data = np_train_trajectories[:,:Nf,:]\n",
    "np_train_target = np_train_trajectories[:,Nf:,:]\n",
    "\n",
    "train_data_tensor = torch.Tensor(np_train_data)\n",
    "train_target_tensor = torch.Tensor(np_train_target)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_tensor, train_target_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print out the trajectory of the first person in the training set. The first Nf (x,y) pairs are for observation. The second Nf (x,y) pairs are those we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      " 0.0120  0.8285\n",
      " 0.0120  0.8285\n",
      " 0.0134  0.8382\n",
      " 0.0219  0.8382\n",
      " 0.0318  0.8405\n",
      " 0.0403  0.8424\n",
      " 0.0502  0.8442\n",
      " 0.0598  0.8465\n",
      " 0.0697  0.8507\n",
      " 0.0792  0.8526\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      " 0.0905  0.8526\n",
      " 0.1011  0.8526\n",
      " 0.1124  0.8549\n",
      " 0.1234  0.8563\n",
      " 0.1347  0.8581\n",
      " 0.1457  0.8600\n",
      " 0.1563  0.8600\n",
      " 0.1676  0.8600\n",
      " 0.1772  0.8563\n",
      " 0.1885  0.8535\n",
      "[torch.FloatTensor of size 10x2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM\n",
    "Now that we have our data loaded into a pytorch dataset and data loader, we are ready to train the LSTM.  \n",
    "\n",
    "We have implemented a TrajectoryPredictor() class that trains an LSTM on the pytorch data loader.\n",
    "TrajectoryPredictor takes as initial parameters: input dimension (2 here because we have x,y inputs), output dimension (2 here because x,y output predictions), and batch size.\n",
    "\n",
    "We now initialize a trajectory predictor and train it. This will take about 1 minute per epoch on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1. mean loss: 1.82455535136\n",
      "epoch 2. mean loss: 0.546421991492\n",
      "epoch 3. mean loss: 0.386153386321\n",
      "epoch 4. mean loss: 0.323843483411\n",
      "epoch 5. mean loss: 0.289107988643\n",
      "epoch 6. mean loss: 0.266656400561\n",
      "epoch 7. mean loss: 0.250179112599\n",
      "epoch 8. mean loss: 0.236991802132\n"
     ]
    }
   ],
   "source": [
    "p = TrajectoryPredictor(2, 2, batch_size)\n",
    "p.train(train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "With our model trained, we predict on some validation trajectories to get a sense how well our LSTM performs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_dev_trajectories = np.stack(dev_trajectories)\n",
    "\n",
    "np_dev_data = np_dev_trajectories[:,:Nf,:]\n",
    "np_dev_target = np_dev_trajectories[:,Nf:,:]\n",
    "\n",
    "dev_data_tensor = torch.Tensor(np_dev_data)\n",
    "dev_target_tensor = torch.Tensor(np_dev_target)\n",
    "\n",
    "dev_dataset = torch.utils.data.TensorDataset(dev_data_tensor, dev_target_tensor)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.534271395503\n"
     ]
    }
   ],
   "source": [
    "p.test(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare with the predictions of a linear model on the same trajectory, where the (x,y) coordinates of a future time frame are estimated by extrapolating from the person's velocity at the final observation time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.128018440883\n"
     ]
    }
   ],
   "source": [
    "compute_linear_error(dev_trajectories, Nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have a ways to go in refining our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "We wish to improve our model soon by incorporating the following into our model:\n",
    "- Presence of other people in the scene\n",
    "- The scene's features (sidewalks, grass, roads, buildings/obstacles)\n",
    "\n",
    "For the presence of other people, we have implemented a data processing step that, for each frame:\n",
    "- looks at all of the positions of the other people in the scene.\n",
    "- constructs an array of 0's and 1's where a 1 indicates the prescence of another person.\n",
    "    - this array is centered at the tracked person's location and is recomputed at each time step.\n",
    "    - the parameter 'N_pixels' gives the number of pixels in each x and y in this discretized representation.\n",
    "We have yet to test this input on the LSTM as we believe it would be more useful to embed this information into a more compact representation, perhaps by first sending through a small CNN for feature extraction.  \n",
    "\n",
    "For the scene information, we wish to:\n",
    "-  Segment the underlying image for each scene into features, labeled by numbers.\n",
    "-  Feed a featurized version of this segmented scene into the LSTM.\n",
    "We hope that including this information will allow the LSTM to learn things such as:\n",
    "-  People usually avoid roads\n",
    "-  People never walk through obstacles\n",
    "-  etc.\n",
    "\n",
    "Eventually, we will incorporate both the information about other people in the scene and the scene features into our LSTM and want to show that including these features improves performance in tracking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
