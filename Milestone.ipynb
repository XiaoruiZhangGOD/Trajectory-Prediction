{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Prediction with LSTM\n",
    "### CS230 - Deep Learning -  Project Milestone. \n",
    "#### Mitchell Dawson, Benjamin Goeing, Tyler Hughes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Project introduction\n",
    " In this project, we are trying to predict the trajectory of traffic objects (people walking, bikers, cars, etc.), as they move and interact with one another.\n",
    " Our model could be used to predict movements of crowds of people and vehicles given an overhead image of a scene," 
    " which may have interest in video surveillance, help make public spaces less susceptible to crowding or accidents, or improve control of autonomous vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports\n",
    "Our LSTM model is built on Pytorch.\n",
    "\n",
    "We will be importing helper modules for processing the dataset and loading an LSTM trajectory tracker class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module import (add here as necessary)\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('./processing_scripts/')\n",
    "sys.path.append('./models/')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from simple_processing import load_simple_array, load_others_array\n",
    "from lstm import TrajectoryPredictor\n",
    "from linear_error import compute_linear_error\n",
    "# import pandas as pd              <- these are used in the data processing step\n",
    "# import matplotlib.pylab as plt   <- \" ... \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "N_pixels = 20   # number of grid points per length in others array\n",
    "Nf = 10         # number of frames to observe before making prediction\n",
    "batch_size = 4  # TrajectoryPredictor training batch size\n",
    "num_epochs = 10 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "We will load in and process the drone dataset.  \n",
    "For now, we will just load (x,y) pairs of positions (normalized between -1 and 1) at a series of frames for each person in the scene.  \n",
    "Our data is taken from the 'stanford' dataset, which contains drone footage from places on Stanford's campus.\n",
    "\n",
    "The data is loaded into a pytorch dataset for feeding into the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories1 = []\n",
    "train_trajectories2 = []\n",
    "for dirname in os.listdir('train_data'):\n",
    "    if dirname == 'stanford':\n",
    "        for filename in os.listdir('train_data/' + dirname):\n",
    "            if filename.endswith('.txt'):\n",
    "                trajectories = load_simple_array('train_data/' + dirname + '/' + filename)\n",
    "                train_trajectories1 += [traj[:Nf,:] for traj in trajectories]\n",
    "                train_trajectories2 += [traj[Nf:,:] for traj in trajectories]\n",
    "\n",
    "train_trajectories1 = np.stack(train_trajectories1)\n",
    "train_trajectories2 = np.stack(train_trajectories2)\n",
    "data_tensor = torch.Tensor(train_trajectories1)\n",
    "target_tensor = torch.Tensor(train_trajectories2)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data_tensor, target_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print out the trajectory of the first person.  \n",
    "The first Nf (x,y) pairs are for observation.\n",
    "The second N-Nf (x,y) pairs are segmented for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      " 0.0120  0.8285\n",
      " 0.0120  0.8285\n",
      " 0.0134  0.8382\n",
      " 0.0219  0.8382\n",
      " 0.0318  0.8405\n",
      " 0.0403  0.8424\n",
      " 0.0502  0.8442\n",
      " 0.0598  0.8465\n",
      " 0.0697  0.8507\n",
      " 0.0792  0.8526\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      " 0.0905  0.8526\n",
      " 0.1011  0.8526\n",
      " 0.1124  0.8549\n",
      " 0.1234  0.8563\n",
      " 0.1347  0.8581\n",
      " 0.1457  0.8600\n",
      " 0.1563  0.8600\n",
      " 0.1676  0.8600\n",
      " 0.1772  0.8563\n",
      " 0.1885  0.8535\n",
      "[torch.FloatTensor of size 10x2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM\n",
    "Now that we have our data loaded into the pytorch dataset and data loader, we are ready to train on the lstm.  \n",
    "\n",
    "We have implemented a TrahectoryPredictor() class that trains an LSTM on the pytorch data loader.\n",
    "TrahectoryPredictor takes as initial parameters:\n",
    "   -input dimension: (2 because we have x,y inputs)\n",
    "   -output dimension:  (2 because we have x,y output predictions)\n",
    "   -batch size: set by 'batch_size' parameter\n",
    "   as\n",
    "\n",
    "We not initialize a trajectory predictore and train it.  \n",
    "This will take about 10 minutes on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0. total loss: 10646.607093\n",
      "epoch: 1. total loss: 5564.71791302\n",
      "epoch: 2. total loss: 5278.94139667\n",
      "epoch: 3. total loss: 5173.01754837\n",
      "epoch: 4. total loss: 5091.45224229\n",
      "epoch: 5. total loss: 5035.86203877\n",
      "epoch: 6. total loss: 4998.51216882\n",
      "epoch: 7. total loss: 4963.22857151\n",
      "epoch: 8. total loss: 4931.85461722\n",
      "epoch: 9. total loss: 4905.97789468\n"
     ]
    }
   ],
   "source": [
    "p = TrajectoryPredictor(2, 2, batch_size)\n",
    "p.train(train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "With our model trained, we predict on some validation trajectories to get a sense how well our LSTM performs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through validation trajectories and print out total error\n",
    "# Mitchell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare with the predictions of a linear model on the same trajectory, where the (x,y) coordinates of a future time frame are estimated by extrapolating from the person's velocity at the final observation time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute linear errors on both simple list and list containing others array (needs work)\n",
    "# Mitchell? what is the best way to do this in your opinion?  I will wait until you've done the train-dev-test split.\n",
    "# Below is the \n",
    "# linear_error_simple = compute_linear_error(trajectories_simple,Nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model outperforms (I hope) the linear case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "We wish to improve our model soon by incorporating the following into our model:\n",
    "- Presence of other people in the scene\n",
    "- The scene's features (sidewalks, grass, roads, buildings/obstacles)\n",
    "\n",
    "For the presence of other people, we have implemented a data processing step that, for each frame:\n",
    "- looks at all of the positions of the other people in the scene.\n",
    "- constructs an array of 0's and 1's where a 1 indicates the prescence of another person.\n",
    "    - this array is centered at the tracked person's location and is recomputed at each time step.\n",
    "    - the parameter 'N_pixels' gives the number of pixels in each x and y in this discretized representation.\n",
    "We have yet to test this input on the LSTM as we believe it would be more useful to embed this information into a more compact representation, perhaps by first sending through a small CNN for feature extraction.  \n",
    "\n",
    "For the scene information, we wish to:\n",
    "-  Segment the underlying image for each scene into features, labeled by numbers.\n",
    "-  Feed a featurized version of this segmented scene into the LSTM.\n",
    "We hope that including this information will allow the LSTM to learn things such as:\n",
    "-  People usually avoid roads\n",
    "-  People never walk through obstacles\n",
    "-  etc.\n",
    "\n",
    "Eventually, we will incorporate both the information about other people in the scene and the scene features into our LSTM and want to show that including these features improves performance in tracking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
